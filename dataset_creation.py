# -*- coding: utf-8 -*-
"""Dataset Creation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PzCHzEPnQjO6ALVZnGOSpMRpfZjBKF2r

# Crawling Twitter Data
"""

import tweepy
import pandas as pd

access_token = "ACCESS_TOKEN"
access_token_secret = "ACCESS_TOKEN_SECRET"
consumer_key = "CONSUMER_KEY"
consumer_key_secret = "CONSUMER_KEY_SECRET"

auth = tweepy.OAuthHandler(consumer_key,consumer_key_secret)
auth.set_access_token(access_token,access_token_secret)
api = tweepy.API(auth,wait_on_rate_limit=True)

#initialize empty list when changing category
created = []
id = []
username = []
text = []
category = []

for tweet in tweepy.Cursor(api.search, 
                           q="keyword -filter:retweets AND -filter:replies AND -filter:quote", 
                           tweet_mode='extended', 
                           lang="id", 
                           since="YYYY-MM-DD", 
                           until ="YYYY-MM-DD").items(1500):
  created.append(tweet.created_at)
  id.append(tweet.id)
  username.append(tweet.user.name)
  text.append(tweet.full_text.encode("utf-8"))
  category.append('category')

dictTweets = {"created_time":created, "id":id, "username":username, "text":text, "class":category}
df = pd.DataFrame(dictTweets)
df = df.drop_duplicates(subset=['text'])

df.to_csv('category.csv', index = False)
df.to_excel('category.xlsx', index = False)

"""# Cleansing Data"""

data = pd.read_csv('category.csv')
data = data[['text','class']].sort_values(['text'], ascending=True, ignore_index=True)

new_data = data.drop([1,2,3,...], axis=0)
new_data = new_data.sample(n=1000, random_state=42)

"""# Splitting Data"""

import numpy as np

train_data, val_data, test_data = np.split(new_data, [int(.70*len(new_data)), int(.9*len(new_data))])

train_data.to_csv('category_train.csv', index = False)
val_data.to_csv('category_val.csv', index = False)
test_data.to_csv('category_test.csv', index = False)

"""# Finishing"""

df1 = pd.read_csv('beasiswa_train.csv')
df2 = pd.read_csv('bulutangkis_train.csv')
df3 = pd.read_csv('demokrasi_train.csv')
df4 = pd.read_csv('film_train.csv')
df5 = pd.read_csv('investasi_train.csv')
df6 = pd.read_csv('kecantikan_train.csv')
df7 = pd.read_csv('konser_train.csv')
df8 = pd.read_csv('pajak_train.csv')
df9 = pd.read_csv('sepakbola_train.csv')
df10 = pd.read_csv('wisata_train.csv')

union_df = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10], ignore_index=True)
union_df = union_df.sample(frac=1).reset_index(drop=True)
union_df.to_csv('train_data.csv', index = False)