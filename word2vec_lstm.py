# -*- coding: utf-8 -*-
"""Word2Vec-LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17XDzJKndIj_nmTiIVud3SG_o7suIAnzu

# Install and Import Library
"""

!pip install tweet-preprocessor
!pip install Sastrawi

import pandas as pd
import nltk
import re
import numpy as np
import preprocessor as p
import tensorflow as tf
import gensim
import matplotlib.pyplot as plt
from google.colab import drive
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.initializers import Constant
from tensorflow.keras.optimizers import Adam
from keras.callbacks import EarlyStopping,ModelCheckpoint
from sklearn.metrics import ConfusionMatrixDisplay,precision_score,recall_score,f1_score

"""# Preparation"""

nltk.download('popular')

drive.mount("/content/drive")

dataset_path = "/content/drive/My Drive/Skripsi Thariq/dataset/"
train = pd.read_csv(dataset_path+"train_data.csv")
validation = pd.read_csv(dataset_path+"val_data.csv")
test = pd.read_csv(dataset_path+"test_data.csv")

X_train = train['text'].values
Y_train = pd.get_dummies(train['class']).values

X_val = validation['text'].values
Y_val = pd.get_dummies(validation['class']).values

X_test = test['text'].values
Y_test = pd.get_dummies(test['class']).values

!unzip "/content/drive/My Drive/Skripsi Thariq/word2vec_id/idwiki_word2vec_300.zip" -d "/content/idwiki_word2vec_300/"

id_w2v = gensim.models.word2vec.Word2Vec.load('idwiki_word2vec_300/idwiki_word2vec_300.model')

slang_txt = open(dataset_path+'Slang.txt',"r",encoding="utf-8", errors='replace')
slang = slang_txt.readlines(); slang_txt.close()
slang = [t.strip('\n').strip() for t in slang]
slang = [t.split(":") for t in slang]
slang = [[k.strip(), v.strip()] for k,v in slang]
slang = {k:v for k,v in slang}

factory = StemmerFactory()
stemmer = factory.create_stemmer()

stopword_txt = open(dataset_path+'Stopwords.txt',"r",encoding="utf-8", errors='replace')
stopword = stopword_txt.readlines(); stopword_txt.close()
stopword = [t.strip('\n').strip() for t in stopword]

"""# Hyperparameter"""

max_seq = 128
embed_dim = 300
lr = 0.001
epochs = 20
batch_size = 64
dropout_prob = 0.2

"""# Text Preprocessing"""

def text_preprocess(sentence):
  sentence = sentence.encode().decode('unicode_escape') #remove escape
  sentence = sentence.lower() #case folding
  sentence = sentence.lstrip('b') #remove first char (b)
  sentence = re.sub(r'\n', ' ', sentence) #remove enter
  sentence = p.clean(sentence) #remove username,hashtag,url,emoji
  sentence = re.sub(r'[^\w\s]', ' ', sentence) #remove punctuation
  sentence = re.sub(r'[0-9]', ' ', sentence) #remove number
  sentence = sentence.split()
  output = []
  for i in range(len(sentence)):
    #remove adjacent duplicate characters
    if re.sub(r'([a-z])\1+', r'\1\1', sentence[i]) in id_w2v.wv.vocab.keys():
      sentence[i] = re.sub(r'([a-z])\1+', r'\1\1', sentence[i])
    else:
      if re.sub(r'([a-z])\1+', r'\1', sentence[i]) in id_w2v.wv.vocab.keys():
        sentence[i] = re.sub(r'([a-z])\1+', r'\1', sentence[i])
      else:
        sentence[i] = re.sub(r'([a-z])\1+', r'\1\1', sentence[i])
    #replace slang and abbreviations into standard language
    if sentence[i] in slang.keys():
      sentence[i] = slang[sentence[i]]
    #stemming
    sentence[i] = stemmer.stem(sentence[i])
    #remove stopwords
    if sentence[i] not in stopword:
      output.append(sentence[i])
  output = ' '.join(output)
  return output

def delete_leaked_word(sentence):
  erased_word = ['beasiswa', 'bulutangkis', 'demokrasi', 'film', 'investasi', 
                 'kecantikan', 'konser', 'pajak', 'sepakbola', 'wisata']
  sentence = sentence.split()
  output = []
  for i in range(len(sentence)):
    if sentence[i] not in erased_word:
      output.append(sentence[i])
  output = ' '.join(output)
  return output

def preprocess_data(data):
  for i in range(len(data)):
    data[i] = text_preprocess(data[i])
    # data[i] = delete_leaked_word(data[i])
  return data

X_train = preprocess_data(X_train)
X_val = preprocess_data(X_val)
X_test = preprocess_data(X_test)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(np.append(X_train, np.append(X_val,X_test)))
vocab_size = len(tokenizer.word_index) + 1

X_train = tokenizer.texts_to_sequences(X_train)
X_val = tokenizer.texts_to_sequences(X_val)
X_test = tokenizer.texts_to_sequences(X_test)

X_train = pad_sequences(X_train, maxlen=max_seq, padding='post')
X_val = pad_sequences(X_val, maxlen=max_seq, padding='post')
X_test = pad_sequences(X_test, maxlen=max_seq, padding='post')

"""# Model Architecture"""

embed_matrix = np.zeros((vocab_size, embed_dim))
for word, i in tokenizer.word_index.items():
  if word in id_w2v.wv.vocab.keys():
    embed_matrix[i] = id_w2v.wv.get_vector(word)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, 
                              output_dim=embed_dim, 
                              input_length=max_seq, 
                              embeddings_initializer=Constant(embed_matrix)),
    tf.keras.layers.Dropout(dropout_prob),
    tf.keras.layers.LSTM(300, return_sequences=True),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(300, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(
  optimizer=Adam(learning_rate=lr),
  loss='categorical_crossentropy',
  metrics=['accuracy']
)

model.summary()

checkpoint_path = "/content/drive/My Drive/Skripsi Thariq/checkpoint/"
callbacks_list = [EarlyStopping(monitor='val_accuracy',
                                patience=5,
                                mode='max',
                                verbose=1),
                  ModelCheckpoint(checkpoint_path+"128_checkpoint_word2vec_lstm",
                                  monitor='val_accuracy',
                                  save_best_only=True,
                                  mode='max',
                                  verbose=1)]

"""# Model Training"""

history = model.fit(X_train, 
                    Y_train,
                    epochs=epochs,
                    batch_size=batch_size, 
                    callbacks=callbacks_list,
                    validation_data=(X_val, Y_val),
                    verbose=1)

"""# Model Evaluation"""

graph_path = "/content/drive/My Drive/Skripsi Thariq/grafik/"

#plot train & validation loss
plt.figure(1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Plot')
plt.ylabel('Value')
plt.xlabel('Epoch')
plt.legend(loc="best")
plt.savefig(graph_path+'Word2Vec-LSTM Train & Validation Loss.png',dpi=300,bbox_inches='tight')

#plot train & validation accuracy
plt.figure(2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Plot')
plt.ylabel('Value')
plt.xlabel('Epoch')
plt.legend(loc="best")
plt.savefig(graph_path+'Word2Vec-LSTM Train & Validation Accuracy.png',dpi=300,bbox_inches='tight')

plt.show()

checkpoint_path = "/content/drive/My Drive/Skripsi Thariq/checkpoint/"
model.load_weights(checkpoint_path+"128_checkpoint_word2vec_lstm")

Y_pred = model.predict(X_test)

plt.figure(3)
cf_matrix = ConfusionMatrixDisplay.from_predictions(Y_test.argmax(axis=1), Y_pred.argmax(axis=1), cmap='Blues')
plt.title('Confusion Matrix')
plt.savefig(graph_path+'Word2Vec-LSTM Confusion Matrix.png',dpi=300,bbox_inches='tight')
plt.show()

print('Precision:', precision_score(Y_test.argmax(axis=1),Y_pred.argmax(axis=1), average='macro'))
print('Recall:', recall_score(Y_test.argmax(axis=1),Y_pred.argmax(axis=1), average='macro'))
print('F1-score:', f1_score(Y_test.argmax(axis=1),Y_pred.argmax(axis=1), average='macro'))

data_test = test['text'].values
label = ['beasiswa', 'bulutangkis', 'demokrasi', 'film', 'investasi', 'kecantikan', 'konser', 'pajak', 'sepakbola', 'wisata']
for i in range(len(data_test)):
  if Y_test.argmax(axis=1)[i] != Y_pred.argmax(axis=1)[i]:
    print('True Label:', label[Y_test.argmax(axis=1)[i]], '|', 'Predicted Label:', label[Y_pred.argmax(axis=1)[i]])
    print('Sentence:', data_test[i], '\n')