# -*- coding: utf-8 -*-
"""IndoBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hqp8fLx89_Mx5ORL2dPE8JBjPdOYdjLv

# Install and Import Library
"""

!pip install tweet-preprocessor
!pip install modelzoo-client[transformers]
!pip install -q keras-rectified-adam

!git lfs install
!git clone https://huggingface.co/indolem/indobert-base-uncased

import pandas as pd
import re
import numpy as np
import preprocessor as p
import tensorflow as tf
import matplotlib.pyplot as plt
from google.colab import drive
from transformers import BertTokenizer, TFBertModel
from keras_radam import RAdam
from keras.callbacks import EarlyStopping,ModelCheckpoint
from sklearn.metrics import ConfusionMatrixDisplay,precision_score,recall_score,f1_score

"""# Preparation"""

drive.mount("/content/drive")

dataset_path = "/content/drive/My Drive/Skripsi Thariq/dataset/"
train = pd.read_csv(dataset_path+"train_data.csv")
validation = pd.read_csv(dataset_path+"val_data.csv")
test = pd.read_csv(dataset_path+"test_data.csv")

X_train = train['text'].values
Y_train = (pd.get_dummies(train['class'])).values

X_val = validation['text'].values
Y_val = (pd.get_dummies(validation['class'])).values

X_test = test['text'].values
Y_test = (pd.get_dummies(test['class'])).values

tokenizer = BertTokenizer.from_pretrained("indolem/indobert-base-uncased")
indobert_model = TFBertModel.from_pretrained("indolem/indobert-base-uncased", from_pt=True)

slang_txt = open(dataset_path+'Slang.txt',"r",encoding="utf-8", errors='replace')
slang = slang_txt.readlines(); slang_txt.close()
slang = [t.strip('\n').strip() for t in slang]
slang = [t.split(":") for t in slang]
slang = [[k.strip(), v.strip()] for k,v in slang]
slang = {k:v for k,v in slang}

"""# Hyperparameter"""

max_seq = 128
lr = 5e-5
epochs = 20
batch_size = 32
dropout_prob = 0.2

"""# Text Preprocessing"""

def text_preprocess(sentence):
  sentence = sentence.encode().decode('unicode_escape') #remove escape
  sentence = sentence.lower() #case folding
  sentence = sentence.lstrip('b') #remove first char (b)
  sentence = re.sub(r'\n', ' ', sentence) #remove enter
  sentence = p.clean(sentence) #remove username,hashtag,url,emoji
  sentence = re.sub(r'[^\w\s]', ' ', sentence) #remove punctuation
  sentence = re.sub(r'[0-9]', ' ', sentence) #remove number
  sentence = sentence.split()
  for i in range(len(sentence)):
    #remove adjacent duplicate characters
    if re.sub(r'([a-z])\1+', r'\1\1', sentence[i]) in tokenizer.vocab.keys():
      sentence[i] = re.sub(r'([a-z])\1+', r'\1\1', sentence[i])
    else:
      if re.sub(r'([a-z])\1+', r'\1', sentence[i]) in tokenizer.vocab.keys():
        sentence[i] = re.sub(r'([a-z])\1+', r'\1', sentence[i])
      else:
        sentence[i] = re.sub(r'([a-z])\1+', r'\1\1', sentence[i])
    #replace slang and abbreviations into standard language
    if sentence[i] in slang.keys():
      sentence[i] = slang[sentence[i]]
  sentence = ' '.join(sentence)
  return sentence

def delete_leaked_word(sentence):
  erased_word = ['beasiswa', 'bulutangkis', 'demokrasi', 'film', 'investasi', 
                 'kecantikan', 'konser', 'pajak', 'sepakbola', 'wisata']
  sentence = sentence.split()
  output = []
  for i in range(len(sentence)):
    if sentence[i] not in erased_word:
      output.append(sentence[i])
  output = ' '.join(output)
  return output

def batch_encode(X, tokenizer):
  return tokenizer.batch_encode_plus(X,
                                     max_length=max_seq, # set the length of the sequences
                                     add_special_tokens=True, # add [CLS] and [SEP] tokens
                                     return_attention_mask=True,
                                     return_token_type_ids=False, # not needed for this type of ML task
                                     padding='max_length', # add 0 pad tokens to the sequences less than max_length
                                     return_tensors='tf')

def preprocess_data(data):
  for i in range(len(data)):
    data[i] = text_preprocess(data[i])
    # data[i] = delete_leaked_word(data[i])
  data = batch_encode(data, tokenizer)
  return data.values()

X_train = preprocess_data(X_train)
X_val = preprocess_data(X_val)
X_test = preprocess_data(X_test)

"""# Model Architecture"""

input_word_ids = tf.keras.layers.Input(shape=(max_seq,), name='input_word_ids', dtype='int32')
input_mask_ids = tf.keras.layers.Input(shape=(max_seq,), name='input_mask_ids', dtype='int32')
indobert = indobert_model([input_word_ids, input_mask_ids])
pooled_output = indobert['pooler_output']
dropout = tf.keras.layers.Dropout(dropout_prob)(pooled_output)
hidden = tf.keras.layers.Dense(768, activation='relu')(dropout)
outputs = tf.keras.layers.Dense(10, activation='softmax')(hidden)
model = tf.keras.models.Model(inputs = [input_word_ids,input_mask_ids], outputs = outputs)

model.compile(
  optimizer=RAdam(learning_rate=lr),
  loss='categorical_crossentropy',
  metrics=['accuracy']
)

model.summary()

checkpoint_path = "/content/drive/My Drive/Skripsi Thariq/checkpoint/"
callbacks_list = [EarlyStopping(monitor='val_accuracy',
                                patience=5,
                                mode='max',
                                verbose=1),
                  ModelCheckpoint(checkpoint_path+"128_checkpoint_indobert",
                                  monitor='val_accuracy',
                                  save_best_only=True,
                                  mode='max',
                                  verbose=1)]

"""# Model Training"""

history = model.fit(X_train, 
                    Y_train,
                    epochs=epochs,
                    batch_size=batch_size, 
                    callbacks=callbacks_list,
                    validation_data=(X_val, Y_val),
                    verbose=1)

"""# Model Evaluation"""

graph_path = "/content/drive/My Drive/Skripsi Thariq/grafik/"

#plot train & validation loss
plt.figure(1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Plot')
plt.ylabel('Value')
plt.xlabel('Epoch')
plt.legend(loc="best")
plt.savefig(graph_path+'IndoBERT Train & Validation Loss.png',dpi=300,bbox_inches='tight')

#plot train & validation accuracy
plt.figure(2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Plot')
plt.ylabel('Value')
plt.xlabel('Epoch')
plt.legend(loc="best")
plt.savefig(graph_path+'IndoBERT Train & Validation Accuracy.png',dpi=300,bbox_inches='tight')

plt.show()

checkpoint_path = "/content/drive/My Drive/Skripsi Thariq/checkpoint/"
model.load_weights(checkpoint_path+"128_checkpoint_indobert")

Y_pred = model.predict(X_test)

plt.figure(3)
cf_matrix = ConfusionMatrixDisplay.from_predictions(Y_test.argmax(axis=1), Y_pred.argmax(axis=1), cmap='Blues')
plt.title('Confusion Matrix')
plt.savefig(graph_path+'IndoBERT Confusion Matrix.png',dpi=300,bbox_inches='tight')
plt.show()

print('Precision:', precision_score(Y_test.argmax(axis=1),Y_pred.argmax(axis=1), average='macro'))
print('Recall:', recall_score(Y_test.argmax(axis=1),Y_pred.argmax(axis=1), average='macro'))
print('F1-score:', f1_score(Y_test.argmax(axis=1),Y_pred.argmax(axis=1), average='macro'))

data_test = test['text'].values
label = ['beasiswa', 'bulutangkis', 'demokrasi', 'film', 'investasi', 'kecantikan', 'konser', 'pajak', 'sepakbola', 'wisata']
for i in range(len(data_test)):
  if Y_test.argmax(axis=1)[i] != Y_pred.argmax(axis=1)[i]:
    print('True Label:', label[Y_test.argmax(axis=1)[i]], '|', 'Predicted Label:', label[Y_pred.argmax(axis=1)[i]])
    print('Sentence:', data_test[i], '\n')